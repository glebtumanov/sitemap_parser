#!/usr/bin/env python
import os
import time
import asyncio
import psycopg2
from psycopg2.extras import execute_values
from openai import AsyncOpenAI
from dotenv import load_dotenv
from tqdm import tqdm
import logging
import configparser
from tabulate import tabulate
from datetime import datetime
import tiktoken

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ª–æ–≥–æ–≤ HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç OpenAI
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BATCH_SIZE = 100  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑
MAX_RETRIES = 3
RETRY_DELAY = 2  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
CONCURRENCY_LIMIT = 5  # –õ–∏–º–∏—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API
CONFIG_PATH = "config/config.ini"
CONFIG_SECTION_DB = "RSS-News.postgres_local"
CONFIG_SECTION_OPENAI = "OpenAI"
CONTENT_MIN_LENGTH = 500
TOTAL_LIMIT = 1000
PRICE_PER_1M_TOKENS = 0.02 # —Ü–µ–Ω–∞ –∑–∞ 1 –º–ª–Ω —Ç–æ–∫–µ–Ω–æ–≤
MAX_CONTENT_LENGTH = 8192 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö

def load_config():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
    config = configparser.ConfigParser()
    config.read(CONFIG_PATH)

    db_config = {
        'host': config[CONFIG_SECTION_DB]['host'],
        'port': config[CONFIG_SECTION_DB]['port'],
        'dbname': config[CONFIG_SECTION_DB]['dbname'],
        'user': config[CONFIG_SECTION_DB]['user'],
        'password': config[CONFIG_SECTION_DB]['password']
    }

    openai_config = {
        'api_key': config[CONFIG_SECTION_OPENAI]['api_key'],
        'model_embeddings': config[CONFIG_SECTION_OPENAI]['model_embeddings']
    }

    return db_config, openai_config

class EmbeddingProcessor:
    def __init__(self, db_config, openai_config):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

        :param db_config: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
        :param openai_config: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ OpenAI
        """
        self.db_config = db_config
        self.openai_config = openai_config
        self.client = AsyncOpenAI(api_key=openai_config['api_key'])
        self.semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.tokenizer = tiktoken.encoding_for_model(openai_config['model_embeddings'])
        if not hasattr(tiktoken.model, openai_config['model_embeddings']):
            # –ï—Å–ª–∏ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º cl100k_base (–¥–ª—è text-embedding-3-*)
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏
        self.stats = {
            'total_texts': 0,
            'processed_texts': 0,
            'successful_embeddings': 0,
            'failed_embeddings': 0,
            'skipped_texts': 0,
            'start_time': datetime.now(),
            'domains_stats': {},
            'tokens_sent': 0,
            'chars_sent': 0,
            'avg_tokens_per_text': 0,
            'avg_chars_per_token': 0,
            'truncated_texts': 0
        }

    def connect_to_db(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î"""
        return psycopg2.connect(**self.db_config)

    def get_unprocessed_contents(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ –µ—â–µ –Ω–µ —Å–æ–∑–¥–∞–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        with self.connect_to_db() as conn:
            with conn.cursor() as cursor:
                query = """
                SELECT npc.id_page, npc.content, s.domain
                FROM news_pages_content npc
                LEFT JOIN content_embeddings ce ON npc.id_page = ce.id_page
                JOIN news_pages np ON npc.id_page = np.id_page
                JOIN sitemaps s ON np.id_sitemap = s.id_sitemap
                WHERE ce.id IS NULL AND npc.content IS NOT NULL AND LENGTH(npc.content) >= %s
                ORDER BY npc.publication_date DESC
                LIMIT %s
                """
                cursor.execute(query, (CONTENT_MIN_LENGTH, TOTAL_LIMIT))
                return cursor.fetchall()

    def truncate_text_to_token_limit(self, text):
        """
        –û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤

        :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        :return: –û–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –¥–ª–∏–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        """
        tokens = self.tokenizer.encode(text)
        original_token_count = len(tokens)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–µ–∑–∞—Ç—å
        if original_token_count <= MAX_CONTENT_LENGTH - 1:
            return text, original_token_count, len(text)

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ MAX_CONTENT_LENGTH - 1 —Ç–æ–∫–µ–Ω–æ–≤
        truncated_tokens = tokens[:MAX_CONTENT_LENGTH - 1]
        truncated_text = self.tokenizer.decode(truncated_tokens)

        self.stats['truncated_texts'] += 1

        return truncated_text, len(truncated_tokens), len(truncated_text)

    async def get_embedding(self, text, id_page, domain):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞

        :param text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        :param id_page: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        :param domain: –î–æ–º–µ–Ω —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        :return: –ö–æ—Ä—Ç–µ–∂ (id_page, embedding)
        """
        async with self.semaphore:
            # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
            processed_text, token_count, char_count = self.truncate_text_to_token_limit(text)

            self.stats['tokens_sent'] += token_count
            self.stats['chars_sent'] += char_count

            for attempt in range(MAX_RETRIES):
                try:
                    response = await self.client.embeddings.create(
                        model=self.openai_config['model_embeddings'],
                        input=processed_text,
                        encoding_format="float"
                    )
                    embedding = response.data[0].embedding

                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    self.stats['successful_embeddings'] += 1
                    if domain not in self.stats['domains_stats']:
                        self.stats['domains_stats'][domain] = {'success': 0, 'failed': 0, 'skipped': 0, 'tokens': 0, 'chars': 0}
                    self.stats['domains_stats'][domain]['success'] += 1
                    self.stats['domains_stats'][domain]['tokens'] = self.stats['domains_stats'][domain].get('tokens', 0) + token_count
                    self.stats['domains_stats'][domain]['chars'] = self.stats['domains_stats'][domain].get('chars', 0) + char_count

                    return id_page, embedding
                except Exception as e:
                    if attempt < MAX_RETRIES - 1:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è id_page={id_page}. –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{MAX_RETRIES}. –û—à–∏–±–∫–∞: {e}")
                        await asyncio.sleep(RETRY_DELAY * (2 ** attempt))  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    else:
                        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è id_page={id_page} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫. –û—à–∏–±–∫–∞: {e}")

                        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–µ—É–¥–∞—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        self.stats['failed_embeddings'] += 1
                        if domain not in self.stats['domains_stats']:
                            self.stats['domains_stats'][domain] = {'success': 0, 'failed': 0, 'skipped': 0, 'tokens': 0, 'chars': 0}
                        self.stats['domains_stats'][domain]['failed'] += 1

                        raise

    async def process_batch(self, batch, pbar):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

        :param batch: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (id_page, content, domain) –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        :param pbar: –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        :return: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (id_page, embedding)
        """
        tasks = []
        for id_page, content, domain in batch:
            if content:  # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—É—Å—Ç–æ–π
                tasks.append(self.get_embedding(content, id_page, domain))
                self.stats['processed_texts'] += 1
            else:
                logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ id_page={id_page} –∏–∑-–∑–∞ –ø—É—Å—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
                self.stats['skipped_texts'] += 1
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –¥–æ–º–µ–Ω–∞
                if domain not in self.stats['domains_stats']:
                    self.stats['domains_stats'][domain] = {'success': 0, 'failed': 0, 'skipped': 0, 'tokens': 0, 'chars': 0}
                self.stats['domains_stats'][domain]['skipped'] += 1
                pbar.update(1)

        results = []
        for future in asyncio.as_completed(tasks):
            try:
                result = await future
                results.append(result)
                pbar.update(1)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}")
                pbar.update(1)

        return results

    def save_embeddings(self, embeddings):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –ë–î

        :param embeddings: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (id_page, embedding)
        """
        if not embeddings:
            return

        with self.connect_to_db() as conn:
            with conn.cursor() as cursor:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
                values = [(id_page, embedding) for id_page, embedding in embeddings]

                # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º execute_values –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                execute_values(
                    cursor,
                    "INSERT INTO content_embeddings (id_page, embedding) VALUES %s ON CONFLICT (id_page) DO UPDATE SET embedding = EXCLUDED.embedding",
                    values,
                    template="(%s, %s)"
                )

            conn.commit()

    async def process_all(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        contents = self.get_unprocessed_contents()
        total = len(contents)
        self.stats['total_texts'] = total

        if total == 0:
            logger.info("–í—Å–µ —Ç–µ–∫—Å—Ç—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            return

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {total} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        with tqdm(total=total, desc="–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤") as pbar:
            # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞–∫–µ—Ç—ã
            for i in range(0, total, BATCH_SIZE):
                batch = contents[i:i+BATCH_SIZE]
                embeddings = await self.process_batch(batch, pbar)

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞–∫–µ—Ç–∞
                self.save_embeddings(embeddings)

    def print_stats(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Å—Å–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        end_time = datetime.now()
        processing_time = (end_time - self.stats['start_time']).total_seconds()

        # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        if self.stats['successful_embeddings'] > 0:
            self.stats['avg_tokens_per_text'] = self.stats['tokens_sent'] / self.stats['successful_embeddings']
        else:
            self.stats['avg_tokens_per_text'] = 0

        if self.stats['tokens_sent'] > 0:
            self.stats['avg_chars_per_token'] = self.stats['chars_sent'] / self.stats['tokens_sent']
        else:
            self.stats['avg_chars_per_token'] = 0

        # –°—Ç–æ–∏–º–æ—Å—Ç—å –≤ –¥–æ–ª–ª–∞—Ä–∞—Ö
        cost = (self.stats['tokens_sent'] / 1_000_000) * PRICE_PER_1M_TOKENS

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        summary_stats = [
            ["–í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", self.stats['total_texts']],
            ["–£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", self.stats['successful_embeddings']],
            ["–ù–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫", self.stats['failed_embeddings']],
            ["–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", self.stats['skipped_texts']],
            ["–û–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", self.stats['truncated_texts']],
            ["–í—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤", f"{self.stats['tokens_sent']:,}".replace(',', ' ')],
            ["–í—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤", f"{self.stats['chars_sent']:,}".replace(',', ' ')],
            ["–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª-–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Ç–µ–∫—Å—Ç", f"{self.stats['avg_tokens_per_text']:.1f}"],
            ["–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª-–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ 1 —Ç–æ–∫–µ–Ω", f"{self.stats['avg_chars_per_token']:.1f}"],
            ["–°—Ç–æ–∏–º–æ—Å—Ç—å API", f"${cost:.6f}"],
            ["–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", f"{processing_time:.2f} —Å–µ–∫"],
            ["–°–∫–æ—Ä–æ—Å—Ç—å", f"{self.stats['successful_embeddings'] / max(processing_time, 1):.2f} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤/—Å–µ–∫"]
        ]

        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ï–°–°–ò–ò –°–û–ó–î–ê–ù–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í")
        print(tabulate(summary_stats, headers=["–ú–µ—Ç—Ä–∏–∫–∞", "–ó–Ω–∞—á–µ–Ω–∏–µ"], tablefmt="pretty",
                      colalign=("left", "right")))

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º
        if self.stats['domains_stats']:
            domains_table = []
            for i, (domain, stats) in enumerate(sorted(self.stats['domains_stats'].items()), 1):
                domain_tokens = stats.get('tokens', 0)
                domain_chars = stats.get('chars', 0)
                domain_cost = (domain_tokens / 1_000_000) * PRICE_PER_1M_TOKENS
                domains_table.append([
                    i, domain,
                    stats.get('success', 0),
                    stats.get('failed', 0),
                    stats.get('skipped', 0),
                    stats.get('success', 0) + stats.get('failed', 0) + stats.get('skipped', 0),
                    f"{domain_tokens:,}".replace(',', ' '),
                    f"{domain_chars:,}".replace(',', ' '),
                    f"${domain_cost:.6f}"
                ])

            print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–û–ú–ï–ù–ê–ú")
            print(tabulate(
                domains_table,
                headers=["‚Ññ", "–î–æ–º–µ–Ω", "–£—Å–ø–µ—à–Ω–æ", "–û—à–∏–±–æ–∫", "–ü—Ä–æ–ø—É—â–µ–Ω–æ", "–í—Å–µ–≥–æ", "–¢–æ–∫–µ–Ω–æ–≤", "–°–∏–º–≤–æ–ª–æ–≤", "–°—Ç–æ–∏–º–æ—Å—Ç—å"],
                tablefmt="pretty",
                colalign=("right", "left", "right", "right", "right", "right", "right", "right", "right")
            ))

def main():
    start_time = time.time()
    processor = None

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞
        db_config, openai_config = load_config()

        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        processor = EmbeddingProcessor(db_config, openai_config)
        asyncio.run(processor.process_all())

        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    except KeyboardInterrupt:
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏: {e}", exc_info=True)
    finally:
        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –µ—Å–ª–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        if processor:
            processor.print_stats()

if __name__ == "__main__":
    main()