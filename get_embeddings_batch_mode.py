#!/usr/bin/env python
import os
import sys
import time
import asyncio
import json
import psycopg2
import argparse
from psycopg2.extras import execute_values
from openai import AsyncOpenAI, OpenAI
from dotenv import load_dotenv
from tqdm import tqdm
import logging
import configparser
from tabulate import tabulate
from datetime import datetime
import tiktoken
import tempfile

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ª–æ–≥–æ–≤ HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç OpenAI
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BATCH_SIZE = 100  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑
MAX_RETRIES = 3
RETRY_DELAY = 2  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
CONCURRENCY_LIMIT = 5  # –õ–∏–º–∏—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API
CONFIG_PATH = "config/config.ini"
CONFIG_SECTION_DB = "RSS-News.postgres_local"
CONFIG_SECTION_OPENAI = "OpenAI"
CONTENT_MIN_LENGTH = 500
TOTAL_LIMIT = 1000
PRICE_PER_1M_TOKENS = 0.02 # —Ü–µ–Ω–∞ –∑–∞ 1 –º–ª–Ω —Ç–æ–∫–µ–Ω–æ–≤
BATCH_CHECK_INTERVAL = 10  # –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –±–∞—Ç—á–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
BATCH_MAX_WAIT_TIME = 24 * 60 * 60  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–∞—Ç—á–∞ (24 —á–∞—Å–∞)
BATCH_INFO_FILE = "batch_jobs.json"  # –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ batch-–∑–∞–¥–∞–Ω–∏—è—Ö

def load_config():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
    config = configparser.ConfigParser()
    config.read(CONFIG_PATH)

    db_config = {
        'host': config[CONFIG_SECTION_DB]['host'],
        'port': config[CONFIG_SECTION_DB]['port'],
        'dbname': config[CONFIG_SECTION_DB]['dbname'],
        'user': config[CONFIG_SECTION_DB]['user'],
        'password': config[CONFIG_SECTION_DB]['password']
    }

    openai_config = {
        'api_key': config[CONFIG_SECTION_OPENAI]['api_key'],
        'model_embeddings': config[CONFIG_SECTION_OPENAI]['model_embeddings']
    }

    return db_config, openai_config

class EmbeddingProcessor:
    def __init__(self, db_config, openai_config):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

        :param db_config: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
        :param openai_config: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ OpenAI
        """
        self.db_config = db_config
        self.openai_config = openai_config
        self.async_client = AsyncOpenAI(api_key=openai_config['api_key'])
        self.sync_client = OpenAI(api_key=openai_config['api_key'])
        self.semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.tokenizer = tiktoken.encoding_for_model(openai_config['model_embeddings'])
        if not hasattr(tiktoken.model, openai_config['model_embeddings']):
            # –ï—Å–ª–∏ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º cl100k_base (–¥–ª—è text-embedding-3-*)
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏
        self.stats = {
            'total_texts': 0,
            'processed_texts': 0,
            'successful_embeddings': 0,
            'failed_embeddings': 0,
            'skipped_texts': 0,
            'start_time': datetime.now(),
            'domains_stats': {},
            'tokens_sent': 0,
            'avg_tokens_per_text': 0,
            'batch_jobs': 0,
            'batch_processing_time': 0
        }

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ batch-–∑–∞–¥–∞–Ω–∏—è—Ö –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        self.batch_jobs = self.load_batch_jobs()

    def connect_to_db(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î"""
        return psycopg2.connect(**self.db_config)

    def load_batch_jobs(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ batch-–∑–∞–¥–∞–Ω–∏—è—Ö –∏–∑ —Ñ–∞–π–ª–∞"""
        if os.path.exists(BATCH_INFO_FILE):
            try:
                with open(BATCH_INFO_FILE, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ batch-–∑–∞–¥–∞–Ω–∏—è—Ö: {e}")
                return {'pending': [], 'completed': [], 'failed': []}
        else:
            return {'pending': [], 'completed': [], 'failed': []}

    def save_batch_jobs(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ batch-–∑–∞–¥–∞–Ω–∏—è—Ö –≤ —Ñ–∞–π–ª"""
        try:
            with open(BATCH_INFO_FILE, 'w') as f:
                json.dump(self.batch_jobs, f, indent=2)
            logger.info(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ batch-–∑–∞–¥–∞–Ω–∏—è—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {BATCH_INFO_FILE}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ batch-–∑–∞–¥–∞–Ω–∏—è—Ö: {e}")

    def get_unprocessed_contents(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ –µ—â–µ –Ω–µ —Å–æ–∑–¥–∞–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        # –ü–æ–ª—É—á–µ–Ω–∏–µ ID —Å—Ç—Ä–∞–Ω–∏—Ü, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã batch-–∑–∞–¥–∞–Ω–∏—è
        pending_ids = []
        for job in self.batch_jobs['pending']:
            pending_ids.extend(job.get('page_ids', []))

        with self.connect_to_db() as conn:
            with conn.cursor() as cursor:
                query = """
                SELECT npc.id_page, npc.content, s.domain
                FROM news_pages_content npc
                LEFT JOIN content_embeddings ce ON npc.id_page = ce.id_page
                JOIN news_pages np ON npc.id_page = np.id_page
                JOIN sitemaps s ON np.id_sitemap = s.id_sitemap
                WHERE ce.id IS NULL
                  AND npc.content IS NOT NULL
                  AND LENGTH(npc.content) >= %s
                  AND npc.id_page NOT IN (
                    SELECT UNNEST(%s::bigint[])
                  )
                ORDER BY npc.publication_date DESC
                LIMIT %s
                """
                cursor.execute(query, (CONTENT_MIN_LENGTH, pending_ids if pending_ids else [0], TOTAL_LIMIT))
                return cursor.fetchall()

    def prepare_batch_file(self, contents):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ JSONL —Ñ–∞–π–ª–∞ –¥–ª—è Batch API

        :param contents: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (id_page, content, domain)
        :return: –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É JSONL —Ñ–∞–π–ª—É –∏ —Å–ª–æ–≤–∞—Ä—å —Å —Ç–æ–∫–µ–Ω–∞–º–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º
        """
        fd, temp_path = tempfile.mkstemp(suffix='.jsonl')

        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–∞—Ö –ø–æ –¥–æ–º–µ–Ω–∞–º
        domain_tokens = {}
        total_tokens = 0
        page_ids = []

        with os.fdopen(fd, 'w') as f:
            for i, (id_page, content, domain) in enumerate(contents):
                if not content:
                    logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ id_page={id_page} –∏–∑-–∑–∞ –ø—É—Å—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
                    self.stats['skipped_texts'] += 1
                    if domain not in self.stats['domains_stats']:
                        self.stats['domains_stats'][domain] = {'success': 0, 'failed': 0, 'skipped': 0, 'tokens': 0}
                    self.stats['domains_stats'][domain]['skipped'] += 1
                    continue

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º ID —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
                page_ids.append(id_page)

                # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
                tokens = self.tokenizer.encode(content)
                token_count = len(tokens)
                total_tokens += token_count

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –¥–æ–º–µ–Ω–∞–º
                if domain not in domain_tokens:
                    domain_tokens[domain] = 0
                domain_tokens[domain] += token_count

                # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è Batch API
                request = {
                    "custom_id": str(id_page),
                    "method": "POST",
                    "url": "/v1/embeddings",
                    "body": {
                        "model": self.openai_config['model_embeddings'],
                        "input": content,
                        "encoding_format": "float"
                    }
                }

                f.write(json.dumps(request) + '\n')
                self.stats['processed_texts'] += 1

        logger.info(f"–°–æ–∑–¥–∞–Ω JSONL —Ñ–∞–π–ª —Å {self.stats['processed_texts']} –∑–∞–ø—Ä–æ—Å–∞–º–∏, –≤—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
        self.stats['tokens_sent'] += total_tokens

        return temp_path, domain_tokens, page_ids

    def upload_batch_file(self, file_path):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ JSONL —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ Files API

        :param file_path: –ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É
        :return: ID –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        """
        try:
            with open(file_path, 'rb') as file:
                response = self.sync_client.files.create(
                    file=file,
                    purpose="batch"
                )
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω, ID: {response.id}")
            return response.id
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
            raise

    def create_batch_job(self, file_id):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ batch-–∑–∞–¥–∞–Ω–∏—è

        :param file_id: ID –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        :return: ID —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ batch-–∑–∞–¥–∞–Ω–∏—è
        """
        try:
            response = self.sync_client.batches.create(
                input_file_id=file_id,
                endpoint="/v1/embeddings",
                completion_window="24h"
            )
            logger.info(f"Batch-–∑–∞–¥–∞–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ, ID: {response.id}, —Å—Ç–∞—Ç—É—Å: {response.status}")
            self.stats['batch_jobs'] += 1
            return response.id
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ batch-–∑–∞–¥–∞–Ω–∏—è: {e}")
            raise

    def wait_for_batch_completion(self, batch_id):
        """
        –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è batch-–∑–∞–¥–∞–Ω–∏—è —Å –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—Ç–∞—Ç—É—Å–∞

        :param batch_id: ID batch-–∑–∞–¥–∞–Ω–∏—è
        :return: –û–±—ä–µ–∫—Ç batch —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        start_time = time.time()
        wait_time = 0

        logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è batch-–∑–∞–¥–∞–Ω–∏—è {batch_id}...")

        while wait_time < BATCH_MAX_WAIT_TIME:
            batch = self.sync_client.batches.retrieve(batch_id)

            if batch.status in ["completed", "failed", "expired", "cancelled"]:
                end_time = time.time()
                processing_time = end_time - start_time
                self.stats['batch_processing_time'] += processing_time

                logger.info(f"Batch-–∑–∞–¥–∞–Ω–∏–µ {batch_id} –∑–∞–≤–µ—Ä—à–µ–Ω–æ —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º {batch.status} –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥")

                if batch.status == "completed":
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {batch.request_counts.completed}/{batch.request_counts.total}")
                elif batch.status in ["failed", "expired", "cancelled"]:
                    logger.error(f"Batch-–∑–∞–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–æ–π. –°—Ç–∞—Ç—É—Å: {batch.status}")
                    if batch.status == "failed" and batch.error_file_id:
                        error_content = self.sync_client.files.content(batch.error_file_id)
                        logger.error(f"–û—à–∏–±–∫–∏: {error_content.text[:500]}...")

                return batch

            wait_time = time.time() - start_time
            logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ... –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å: {batch.status}, –ø—Ä–æ—à–ª–æ {wait_time:.2f} —Å–µ–∫—É–Ω–¥")
            time.sleep(BATCH_CHECK_INTERVAL)

        # –ï—Å–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
        logger.error(f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è ({BATCH_MAX_WAIT_TIME} —Å–µ–∫—É–Ω–¥) –¥–ª—è batch-–∑–∞–¥–∞–Ω–∏—è {batch_id}")
        return None

    def check_batch_status(self, batch_id):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ batch-–∑–∞–¥–∞–Ω–∏—è –±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è

        :param batch_id: ID batch-–∑–∞–¥–∞–Ω–∏—è
        :return: –°—Ç–∞—Ç—É—Å –∑–∞–¥–∞–Ω–∏—è –∏ –æ–±—ä–µ–∫—Ç batch
        """
        try:
            batch = self.sync_client.batches.retrieve(batch_id)
            return batch.status, batch
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Ç–∞—Ç—É—Å–∞ batch-–∑–∞–¥–∞–Ω–∏—è {batch_id}: {e}")
            return "error", None

    def process_batch_results(self, batch, domain_tokens):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ batch-–∑–∞–¥–∞–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –ë–î

        :param batch: –û–±—ä–µ–∫—Ç batch —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        :param domain_tokens: –°–ª–æ–≤–∞—Ä—å —Å —Ç–æ–∫–µ–Ω–∞–º–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º
        :return: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        if not batch or not batch.output_file_id:
            logger.error("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return 0

        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            output_content = self.sync_client.files.content(batch.output_file_id)
            output_text = output_content.text

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            error_data = {}
            if batch.error_file_id:
                error_content = self.sync_client.files.content(batch.error_file_id)
                for line in error_content.text.strip().split('\n'):
                    error_obj = json.loads(line)
                    error_data[error_obj['custom_id']] = error_obj['error']

            # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            embeddings = []
            successful_count = 0
            failed_count = 0

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            with self.connect_to_db() as conn:
                with conn.cursor() as cursor:
                    for line in output_text.strip().split('\n'):
                        result = json.loads(line)
                        custom_id = result['custom_id']
                        id_page = int(custom_id)

                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ—à–∏–±–∫–∏
                        if result.get('error') or result['response']['status_code'] != 200:
                            failed_count += 1
                            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è id_page={id_page}: {result.get('error')}")
                            continue

                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                        response_body = result['response']['body']
                        embedding = response_body['data'][0]['embedding']

                        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –≤—Å—Ç–∞–≤–∫–∏
                        embeddings.append((id_page, embedding))
                        successful_count += 1

                        # –ü–∞–∫–µ—Ç–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –∫–∞–∂–¥—ã–µ 100 —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                        if len(embeddings) >= 100:
                            execute_values(
                                cursor,
                                "INSERT INTO content_embeddings (id_page, embedding) VALUES %s ON CONFLICT (id_page) DO UPDATE SET embedding = EXCLUDED.embedding",
                                embeddings,
                                template="(%s, %s)"
                            )
                            conn.commit()
                            embeddings = []

                    # –í—Å—Ç–∞–≤–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                    if embeddings:
                        execute_values(
                            cursor,
                            "INSERT INTO content_embeddings (id_page, embedding) VALUES %s ON CONFLICT (id_page) DO UPDATE SET embedding = EXCLUDED.embedding",
                            embeddings,
                            template="(%s, %s)"
                        )
                        conn.commit()

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats['successful_embeddings'] += successful_count
            self.stats['failed_embeddings'] += failed_count

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º
            for domain, tokens in domain_tokens.items():
                if domain not in self.stats['domains_stats']:
                    self.stats['domains_stats'][domain] = {'success': 0, 'failed': 0, 'skipped': 0, 'tokens': 0}

                # –ü—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å–ø–µ—Ö–∏ –∏ –Ω–µ—É–¥–∞—á–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –¥–æ–ª–∏ —Ç–æ–∫–µ–Ω–æ–≤
                domain_ratio = tokens / sum(domain_tokens.values()) if sum(domain_tokens.values()) > 0 else 0
                domain_success = round(successful_count * domain_ratio)
                domain_failed = round(failed_count * domain_ratio)

                self.stats['domains_stats'][domain]['success'] += domain_success
                self.stats['domains_stats'][domain]['failed'] += domain_failed
                self.stats['domains_stats'][domain]['tokens'] += tokens

            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {successful_count} —É—Å–ø–µ—à–Ω–æ, {failed_count} —Å –æ—à–∏–±–∫–∞–º–∏")
            return successful_count

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            return 0

    async def create_batches(self):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ batch-–∑–∞–¥–∞–Ω–∏–π –±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        contents = self.get_unprocessed_contents()
        total = len(contents)
        self.stats['total_texts'] = total

        if total == 0:
            logger.info("–í—Å–µ —Ç–µ–∫—Å—Ç—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏–ª–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ—á–µ—Ä–µ–¥–∏ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É")
            return

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {total} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ Batch API")

        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞–∫–µ—Ç—ã
        for i in range(0, total, BATCH_SIZE):
            batch_contents = contents[i:i+BATCH_SIZE]
            logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–∞ {i//BATCH_SIZE + 1}/{(total+BATCH_SIZE-1)//BATCH_SIZE} ({len(batch_contents)} —Ç–µ–∫—Å—Ç–æ–≤)")

            try:
                # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ JSONL —Ñ–∞–π–ª–∞
                batch_file_path, domain_tokens, page_ids = self.prepare_batch_file(batch_contents)

                # –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
                file_id = self.upload_batch_file(batch_file_path)

                # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ batch-–∑–∞–¥–∞–Ω–∏—è
                batch_id = self.create_batch_job(file_id)

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–¥–∞–Ω–∏–∏
                job_info = {
                    'batch_id': batch_id,
                    'input_file_id': file_id,
                    'created_at': datetime.now().isoformat(),
                    'status': 'pending',
                    'page_ids': page_ids,
                    'domain_tokens': domain_tokens
                }

                self.batch_jobs['pending'].append(job_info)
                self.save_batch_jobs()

                # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                try:
                    os.remove(batch_file_path)
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {batch_file_path}: {e}")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ batch-–∑–∞–¥–∞–Ω–∏—è: {e}")

        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {self.stats['batch_jobs']} batch-–∑–∞–¥–∞–Ω–∏–π")

    async def check_pending_batches(self):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –æ–∂–∏–¥–∞—é—â–∏—Ö batch-–∑–∞–¥–∞–Ω–∏–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö
        """
        if not self.batch_jobs['pending']:
            logger.info("–ù–µ—Ç –æ–∂–∏–¥–∞—é—â–∏—Ö batch-–∑–∞–¥–∞–Ω–∏–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
            return

        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ {len(self.batch_jobs['pending'])} –æ–∂–∏–¥–∞—é—â–∏—Ö batch-–∑–∞–¥–∞–Ω–∏–π")

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —Å–ø–∏—Å–∫–∞, —Ç.–∫. –±—É–¥–µ–º –µ–≥–æ –∏–∑–º–µ–Ω—è—Ç—å
        pending_jobs = self.batch_jobs['pending'].copy()

        for job in pending_jobs:
            batch_id = job['batch_id']
            status, batch = self.check_batch_status(batch_id)

            logger.info(f"–°—Ç–∞—Ç—É—Å –∑–∞–¥–∞–Ω–∏—è {batch_id}: {status}")

            if status in ["completed", "failed", "expired", "cancelled"]:
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞–Ω–∏–∏
                job['status'] = status
                job['completed_at'] = datetime.now().isoformat()

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π
                if status == "completed":
                    result_count = self.process_batch_results(batch, job['domain_tokens'])
                    job['processed_count'] = result_count
                    self.batch_jobs['completed'].append(job)
                else:
                    self.batch_jobs['failed'].append(job)

                # –£–¥–∞–ª—è–µ–º –∏–∑ —Å–ø–∏—Å–∫–∞ –æ–∂–∏–¥–∞—é—â–∏—Ö
                self.batch_jobs['pending'].remove(job)

                logger.info(f"–ó–∞–¥–∞–Ω–∏–µ {batch_id} –ø–µ—Ä–µ–º–µ—â–µ–Ω–æ –≤ —Å–ø–∏—Å–æ–∫ {status}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        self.save_batch_jobs()

        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û—Å—Ç–∞–ª–æ—Å—å –æ–∂–∏–¥–∞—é—â–∏—Ö –∑–∞–¥–∞–Ω–∏–π: {len(self.batch_jobs['pending'])}")

    async def process_all(self, mode='full'):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã

        :param mode: –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã ('full', 'create' –∏–ª–∏ 'check')
        """
        if mode == 'create':
            # –¢–æ–ª—å–∫–æ —Å–æ–∑–¥–∞–Ω–∏–µ batch-–∑–∞–¥–∞–Ω–∏–π
            await self.create_batches()
        elif mode == 'check':
            # –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞–Ω–∏–π
            await self.check_pending_batches()
        else:
            # –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: —Å–æ–∑–¥–∞–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞
            await self.create_batches()
            await self.check_pending_batches()

    async def process_batch(self, contents):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Batch API –≤ —Ä–µ–∂–∏–º–µ –æ–∂–∏–¥–∞–Ω–∏—è
        (—É—Å—Ç–∞—Ä–µ–≤—à–∏–π –º–µ—Ç–æ–¥, –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)

        :param contents: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (id_page, content, domain) –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        :return: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        if not contents:
            logger.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return 0

        try:
            # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ JSONL —Ñ–∞–π–ª–∞
            batch_file_path, domain_tokens, _ = self.prepare_batch_file(contents)

            # –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
            file_id = self.upload_batch_file(batch_file_path)

            # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ batch-–∑–∞–¥–∞–Ω–∏—è
            batch_id = self.create_batch_job(file_id)

            # –®–∞–≥ 4: –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            batch = self.wait_for_batch_completion(batch_id)

            # –®–∞–≥ 5: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            result_count = self.process_batch_results(batch, domain_tokens)

            # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            try:
                os.remove(batch_file_path)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {batch_file_path}: {e}")

            return result_count

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞–∫–µ—Ç–∞ —á–µ—Ä–µ–∑ Batch API: {e}")
            return 0

    async def get_embedding(self, text, id_page, domain):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (–º–µ—Ç–æ–¥ –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)

        :param text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        :param id_page: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        :param domain: –î–æ–º–µ–Ω —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        :return: –ö–æ—Ä—Ç–µ–∂ (id_page, embedding)
        """
        async with self.semaphore:
            # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
            tokens = self.tokenizer.encode(text)
            token_count = len(tokens)
            self.stats['tokens_sent'] += token_count

            for attempt in range(MAX_RETRIES):
                try:
                    response = await self.async_client.embeddings.create(
                        model=self.openai_config['model_embeddings'],
                        input=text,
                        encoding_format="float"
                    )
                    embedding = response.data[0].embedding

                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    self.stats['successful_embeddings'] += 1
                    if domain not in self.stats['domains_stats']:
                        self.stats['domains_stats'][domain] = {'success': 0, 'failed': 0, 'skipped': 0, 'tokens': 0}
                    self.stats['domains_stats'][domain]['success'] += 1
                    self.stats['domains_stats'][domain]['tokens'] = self.stats['domains_stats'][domain].get('tokens', 0) + token_count

                    return id_page, embedding
                except Exception as e:
                    if attempt < MAX_RETRIES - 1:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è id_page={id_page}. –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{MAX_RETRIES}. –û—à–∏–±–∫–∞: {e}")
                        await asyncio.sleep(RETRY_DELAY * (2 ** attempt))  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    else:
                        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è id_page={id_page} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫. –û—à–∏–±–∫–∞: {e}")

                        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–µ—É–¥–∞—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        self.stats['failed_embeddings'] += 1
                        if domain not in self.stats['domains_stats']:
                            self.stats['domains_stats'][domain] = {'success': 0, 'failed': 0, 'skipped': 0, 'tokens': 0}
                        self.stats['domains_stats'][domain]['failed'] += 1

                        raise

    def save_embeddings(self, embeddings):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –ë–î

        :param embeddings: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (id_page, embedding)
        """
        if not embeddings:
            return

        with self.connect_to_db() as conn:
            with conn.cursor() as cursor:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
                values = [(id_page, embedding) for id_page, embedding in embeddings]

                # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º execute_values –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                execute_values(
                    cursor,
                    "INSERT INTO content_embeddings (id_page, embedding) VALUES %s ON CONFLICT (id_page) DO UPDATE SET embedding = EXCLUDED.embedding",
                    values,
                    template="(%s, %s)"
                )

            conn.commit()

    def print_stats(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Å—Å–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        end_time = datetime.now()
        processing_time = (end_time - self.stats['start_time']).total_seconds()

        # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        if self.stats['successful_embeddings'] > 0:
            self.stats['avg_tokens_per_text'] = self.stats['tokens_sent'] / self.stats['successful_embeddings']
        else:
            self.stats['avg_tokens_per_text'] = 0

        # –°—Ç–æ–∏–º–æ—Å—Ç—å –≤ –¥–æ–ª–ª–∞—Ä–∞—Ö (—Å —É—á–µ—Ç–æ–º 50% —Å–∫–∏–¥–∫–∏ –¥–ª—è Batch API)
        cost = (self.stats['tokens_sent'] / 1_000_000) * PRICE_PER_1M_TOKENS * 0.5  # 50% —Å–∫–∏–¥–∫–∞ –¥–ª—è Batch API

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        summary_stats = [
            ["–í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", self.stats['total_texts']],
            ["–£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", self.stats['successful_embeddings']],
            ["–ù–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫", self.stats['failed_embeddings']],
            ["–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", self.stats['skipped_texts']],
            ["–í—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤", f"{self.stats['tokens_sent']:,}".replace(',', ' ')],
            ["–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª-–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Ç–µ–∫—Å—Ç", f"{self.stats['avg_tokens_per_text']:.1f}"],
            ["–°—Ç–æ–∏–º–æ—Å—Ç—å API (—Å 50% —Å–∫–∏–¥–∫–æ–π Batch API)", f"${cost:.6f}"],
            ["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ batch-–∑–∞–¥–∞–Ω–∏–π", self.stats['batch_jobs']],
            ["–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ batch-–∑–∞–¥–∞–Ω–∏–π", f"{self.stats['batch_processing_time']:.2f} —Å–µ–∫"],
            ["–û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", f"{processing_time:.2f} —Å–µ–∫"],
            ["–°–∫–æ—Ä–æ—Å—Ç—å", f"{self.stats['successful_embeddings'] / max(processing_time, 1):.2f} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤/—Å–µ–∫"]
        ]

        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ï–°–°–ò–ò –°–û–ó–î–ê–ù–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í")
        print(tabulate(summary_stats, headers=["–ú–µ—Ç—Ä–∏–∫–∞", "–ó–Ω–∞—á–µ–Ω–∏–µ"], tablefmt="pretty"))

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º
        if self.stats['domains_stats']:
            domains_table = []
            for i, (domain, stats) in enumerate(sorted(self.stats['domains_stats'].items()), 1):
                domain_tokens = stats.get('tokens', 0)
                domain_cost = (domain_tokens / 1_000_000) * PRICE_PER_1M_TOKENS * 0.5  # 50% —Å–∫–∏–¥–∫–∞ –¥–ª—è Batch API
                domains_table.append([
                    i, domain,
                    stats.get('success', 0),
                    stats.get('failed', 0),
                    stats.get('skipped', 0),
                    stats.get('success', 0) + stats.get('failed', 0) + stats.get('skipped', 0),
                    f"{domain_tokens:,}".replace(',', ' '),
                    f"${domain_cost:.6f}"
                ])

            print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–û–ú–ï–ù–ê–ú")
            print(tabulate(
                domains_table,
                headers=["‚Ññ", "–î–æ–º–µ–Ω", "–£—Å–ø–µ—à–Ω–æ", "–û—à–∏–±–æ–∫", "–ü—Ä–æ–ø—É—â–µ–Ω–æ", "–í—Å–µ–≥–æ", "–¢–æ–∫–µ–Ω–æ–≤", "–°—Ç–æ–∏–º–æ—Å—Ç—å"],
                tablefmt="pretty",
                colalign=("right", "left", "right", "right", "right", "right", "right", "right")
            ))

    def print_batch_status(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ batch-–∑–∞–¥–∞–Ω–∏—è–º"""
        if not any([self.batch_jobs['pending'], self.batch_jobs['completed'], self.batch_jobs['failed']]):
            print("\n–ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ batch-–∑–∞–¥–∞–Ω–∏—è—Ö")
            return

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º
        status_counts = {
            'pending': len(self.batch_jobs['pending']),
            'completed': len(self.batch_jobs['completed']),
            'failed': len(self.batch_jobs['failed'])
        }

        status_table = []
        for status, count in status_counts.items():
            status_table.append([status, count])

        print("\nüìä –°–¢–ê–¢–£–° BATCH-–ó–ê–î–ê–ù–ò–ô")
        print(tabulate(status_table, headers=["–°—Ç–∞—Ç—É—Å", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"], tablefmt="pretty"))

        # –°–ø–∏—Å–æ–∫ –æ–∂–∏–¥–∞—é—â–∏—Ö –∑–∞–¥–∞–Ω–∏–π
        if self.batch_jobs['pending']:
            pending_table = []
            for i, job in enumerate(self.batch_jobs['pending'], 1):
                created_at = datetime.fromisoformat(job['created_at'])
                elapsed = (datetime.now() - created_at).total_seconds() / 3600  # –≤ —á–∞—Å–∞—Ö
                pending_table.append([
                    i,
                    job['batch_id'],
                    f"{elapsed:.1f} —á",
                    len(job['page_ids'])
                ])

            print("\n‚è≥ –û–ñ–ò–î–ê–Æ–©–ò–ï BATCH-–ó–ê–î–ê–ù–ò–Ø")
            print(tabulate(
                pending_table,
                headers=["‚Ññ", "ID –∑–∞–¥–∞–Ω–∏—è", "–í—Ä–µ–º—è –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ", "–ö–æ–ª-–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü"],
                tablefmt="pretty"
            ))

def parse_args():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description='–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ Batch API OpenAI')
    parser.add_argument('--mode', type=str, choices=['full', 'create', 'check', 'status'], default='full',
                        help='–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: full (–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª), create (—Ç–æ–ª—å–∫–æ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞–Ω–∏–π), '
                             'check (–ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤), status (–≤—ã–≤–æ–¥ —Ç–µ–∫—É—â–µ–≥–æ —Å—Ç–∞—Ç—É—Å–∞)')
    return parser.parse_args()

def main():
    args = parse_args()
    start_time = time.time()
    processor = None

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞
        db_config, openai_config = load_config()

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        processor = EmbeddingProcessor(db_config, openai_config)

        if args.mode == 'status':
            # –¢–æ–ª—å–∫–æ –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞ batch-–∑–∞–¥–∞–Ω–∏–π
            processor.print_batch_status()
        else:
            # –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ
            asyncio.run(processor.process_all(args.mode))
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ä–µ–∂–∏–º–µ '{args.mode}' –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

            # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            processor.print_stats()
            processor.print_batch_status()

    except KeyboardInterrupt:
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏: {e}", exc_info=True)
    finally:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ batch-–∑–∞–¥–∞–Ω–∏—è—Ö, –µ—Å–ª–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        if processor:
            processor.save_batch_jobs()

if __name__ == "__main__":
    main()
