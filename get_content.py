#!/usr/bin/env python
import os
import sys
import json
import configparser
import psycopg2
import requests
from datetime import datetime
from urllib.parse import urlparse
from loguru import logger
import time
import random
from tabulate import tabulate
from collections import defaultdict
import concurrent.futures

# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
import trafilatura

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
NUM_PAGES_FROM_DOMAIN = 1000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞
CONFIG_PATH = "config/config.ini"
COOKIES_DIR = "cookies"  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ñ–∞–π–ª–∞–º–∏ cookie
LOGS_DIR = "logs"  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–æ–≤
MAX_RETRIES = 3  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
MAX_WORKERS = 10  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
MAX_CONCURRENT_PER_DOMAIN = 2  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –æ–¥–Ω–æ–º—É –¥–æ–º–µ–Ω—É
MAX_REDIRECTS = 5  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
REQUEST_TIMEOUT = 40  # –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
stats = {
    'start_time': None,
    'end_time': None,
    'total_raw_pages': 0,
    'successful_raw_pages': 0,
    'failed_raw_pages': 0,
    'total_parsed_pages': 0,
    'total_content_size': 0,  # –≤ –±–∞–π—Ç–∞—Ö
    'domains_stats': defaultdict(lambda: {'success': 0, 'failed': 0, 'size': 0}),
    'parser_stats': {
        'trafilatura': {'success': 0, 'failed': 0, 'parsed': 0}
    }
}

# ---------------------------------------------------
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# ---------------------------------------------------

def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º loguru"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ª–æ–≥–æ–≤
    if not os.path.exists(LOGS_DIR):
        os.makedirs(LOGS_DIR)

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ loguru
    logger.remove()

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª—å —Å —Ü–≤–µ—Ç–∞–º–∏
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        level="INFO",
        colorize=True
    )

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª —Å —Ä–æ—Ç–∞—Ü–∏–µ–π (—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 –∑–∞–ø—É—Å–∫–æ–≤)
    log_file_path = os.path.join(LOGS_DIR, "page_fetcher_{time}.log")
    logger.add(
        log_file_path,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        level="INFO",
        rotation="1 day",
        retention=10
    )

    return logger

def load_config(config_path=CONFIG_PATH):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Configuration file not found: {config_path}")

    config = configparser.ConfigParser()
    config.read(config_path)

    if 'RSS-News.postgres_local' not in config:
        raise KeyError("Section 'RSS-News.postgres_local' not found in configuration file")

    return config['RSS-News.postgres_local']

# ---------------------------------------------------
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
# ---------------------------------------------------

def extract_content(html_content):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Trafilatura"""
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    extracted_text = trafilatura.extract(
        html_content,
        favor_precision=True,
        include_links=False,
        include_images=False,
        include_tables=True,
        include_comments=False,
        output_format='markdown',
        deduplicate=True
    )

    if not extracted_text:
        stats['parser_stats']['trafilatura']['failed'] += 1
        return None

    stats['parser_stats']['trafilatura']['success'] += 1
    return extracted_text

# ---------------------------------------------------
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö
# ---------------------------------------------------

def get_db_connection(db_config):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    conn = psycopg2.connect(
        host=db_config['host'],
        port=db_config['port'],
        dbname=db_config['dbname'],
        user=db_config['user'],
        password=db_config['password']
    )
    return conn

def get_pages_for_raw_content(conn):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—ã—Ä–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    –ù–µ –±–æ–ª–µ–µ NUM_PAGES_FROM_DOMAIN —Å—Ç—Ä–∞–Ω–∏—Ü —Å –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞,
    –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ publication_date –ø–æ —É–±—ã–≤–∞–Ω–∏—é
    """
    query = """
    WITH ranked_pages AS (
        SELECT
            np.id_page,
            np.page_url,
            s.domain,
            np.publication_date,
            ROW_NUMBER() OVER (PARTITION BY s.domain ORDER BY np.publication_date DESC) as rank
        FROM
            news_pages np
        JOIN
            sitemaps s ON np.id_sitemap = s.id_sitemap
        LEFT JOIN
            news_pages_content npc ON np.id_page = npc.id_page
        WHERE
            (npc.id_page IS NULL OR npc.page_raw_content IS NULL)
            AND np.is_error = FALSE
    )
    SELECT
        id_page,
        page_url,
        domain,
        publication_date
    FROM
        ranked_pages
    WHERE
        rank <= %s
    ORDER BY
        publication_date DESC;
    """

    try:
        with conn.cursor() as cursor:
            cursor.execute(query, (NUM_PAGES_FROM_DOMAIN,))
            pages = cursor.fetchall()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        result = []
        for page in pages:
            result.append({
                'id_page': page[0],
                'page_url': page[1],
                'domain': page[2],
                'publication_date': page[3]
            })

        return result
    except Exception as e:
        logger.error(f"Error fetching pages list for raw content: {e}")
        raise

def get_pages_for_content_extraction(conn):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    –í—ã–±–∏—Ä–∞—é—Ç—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —Å—ã—Ä–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç,
    –Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç
    """
    query = """
    SELECT
        npc.id_page,
        COALESCE(npc.page_url, np.page_url) as page_url,
        s.domain,
        COALESCE(npc.publication_date, np.publication_date) as publication_date
    FROM
        news_pages_content npc
    JOIN
        news_pages np ON npc.id_page = np.id_page
    JOIN
        sitemaps s ON np.id_sitemap = s.id_sitemap
    WHERE
        npc.page_raw_content IS NOT NULL
        AND npc.content IS NULL
        AND np.is_error = FALSE
    ORDER BY
        publication_date DESC;
    """

    try:
        with conn.cursor() as cursor:
            cursor.execute(query)
            pages = cursor.fetchall()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        result = []
        for page in pages:
            result.append({
                'id_page': page[0],
                'page_url': page[1],
                'domain': page[2],
                'publication_date': page[3]
            })

        return result
    except Exception as e:
        logger.error(f"Error fetching pages list for content extraction: {e}")
        raise

def save_raw_content_to_db(conn, content_info):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    insert_query = """
    INSERT INTO news_pages_content
        (id_page, page_url, publication_date, page_raw_content)
    VALUES
        (%s, %s, %s, %s)
    ON CONFLICT (id_page) DO UPDATE SET
        page_url = EXCLUDED.page_url,
        publication_date = EXCLUDED.publication_date,
        page_raw_content = EXCLUDED.page_raw_content,
        fetched_at = NOW()
    """

    try:
        with conn.cursor() as cursor:
            cursor.execute(
                insert_query,
                (
                    content_info['id_page'],
                    content_info.get('page_url'),
                    content_info.get('publication_date'),
                    content_info['page_raw_content']
                )
            )
        conn.commit()
        status = "successfully" if content_info['page_raw_content'] else "with error"
        logger.info(f"Raw content for page ID={content_info['id_page']} saved to DB {status}")
        return True
    except Exception as e:
        conn.rollback()
        logger.error(f"Error saving raw content to DB: {e}")
        return False

def get_raw_content_from_db(conn, id_page):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—ã—Ä–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    query = """
    SELECT
        page_raw_content,
        content
    FROM
        news_pages_content
    WHERE
        id_page = %s
    """

    try:
        with conn.cursor() as cursor:
            cursor.execute(query, (id_page,))
            result = cursor.fetchone()

        if not result:
            logger.error(f"No raw content found for page ID={id_page}")
            return None, None

        return result[0], result[1]
    except Exception as e:
        logger.error(f"Error fetching raw content from DB: {e}")
        return None, None

def save_extracted_content_to_db(conn, content_info):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    update_query = """
    UPDATE news_pages_content
    SET
        content = %s
    WHERE
        id_page = %s
    """

    try:
        with conn.cursor() as cursor:
            cursor.execute(
                update_query,
                (
                    content_info.get('content'),
                    content_info['id_page']
                )
            )
        conn.commit()
        logger.info(f"Extracted content for page ID={content_info['id_page']} saved to DB")
        return True
    except Exception as e:
        conn.rollback()
        logger.error(f"Error saving extracted content to DB: {e}")
        return False

def update_page_error_status(conn, id_page, is_error):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ—à–∏–±–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    update_query = """
    UPDATE news_pages
    SET
        is_error = %s
    WHERE
        id_page = %s
    """

    try:
        with conn.cursor() as cursor:
            cursor.execute(
                update_query,
                (
                    is_error,
                    id_page
                )
            )
        conn.commit()
        logger.info(f"Error status for page ID={id_page} updated to {is_error}")
        return True
    except Exception as e:
        conn.rollback()
        logger.error(f"Error updating error status in DB: {e}")
        return False

# ---------------------------------------------------
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å HTTP –∏ cookies
# ---------------------------------------------------

def extract_domain_from_url(url):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –∏–∑ URL"""
    parsed_url = urlparse(url)
    domain = parsed_url.netloc

    # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å www. –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if domain.startswith('www.'):
        domain = domain[4:]

    return domain

def load_cookies(domain):
    """–ó–∞–≥—Ä—É–∑–∫–∞ cookies –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —Ñ–∞–π–ª"""
    cookie_files = []

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    if not os.path.exists(COOKIES_DIR):
        logger.warning(f"Cookies directory not found: {COOKIES_DIR}")
        return []

    # –ò—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞
    for filename in os.listdir(COOKIES_DIR):
        if filename.startswith(f"{domain}_") and filename.endswith('.json'):
            cookie_files.append(filename)

    if not cookie_files:
        logger.warning(f"No cookie files found for domain: {domain}")
        return []

    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã –∏–∑ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤
    date_files = []
    for filename in cookie_files:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (—Ñ–æ—Ä–º–∞—Ç domain_DD-MM-YYYY.json)
        date_str = filename.replace(f"{domain}_", "").replace(".json", "")
        try:
            file_date = datetime.strptime(date_str, "%d-%m-%Y")
            date_files.append((file_date, filename))
        except ValueError:
            logger.warning(f"Invalid date format in cookie file: {filename}")
            continue

    if not date_files:
        logger.warning(f"No valid cookie files with dates found for domain: {domain}")
        return []

    # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —Ñ–∞–π–ª
    latest_file = max(date_files, key=lambda x: x[0])[1]
    cookie_path = os.path.join(COOKIES_DIR, latest_file)

    logger.info(f"Using cookie file: {latest_file}")

    try:
        with open(cookie_path, 'r') as f:
            cookies = json.load(f)
        return cookies
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"Error loading cookie file {latest_file}: {e}")
        return []

def cookies_to_dict(cookies_list):
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ cookies –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è requests"""
    cookies_dict = {}
    for cookie in cookies_list:
        cookies_dict[cookie['name']] = cookie['value']
    return cookies_dict

def get_default_headers():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö HTTP –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0'
    }

def fetch_page_content(page_info, retry=0, redirect_count=0):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ HTML-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤"""
    url = page_info['page_url']
    domain = page_info.get('domain')

    # –ï—Å–ª–∏ –¥–æ–º–µ–Ω –Ω–µ —É–∫–∞–∑–∞–Ω, –∏–∑–≤–ª–µ–∫–∞–µ–º –µ–≥–æ –∏–∑ URL
    if not domain:
        domain = extract_domain_from_url(url)

    # –ó–∞–≥—Ä—É–∑–∫–∞ cookies
    cookies_list = load_cookies(domain)
    cookies_dict = cookies_to_dict(cookies_list)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    headers = get_default_headers()

    try:
        # –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º
        response = requests.get(
            url,
            cookies=cookies_dict,
            headers=headers,
            timeout=REQUEST_TIMEOUT,
            allow_redirects=False
        )

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –≤—Ä—É—á–Ω—É—é
        if response.is_redirect and redirect_count < MAX_REDIRECTS:
            redirect_url = response.headers.get('Location')

            # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω—É—é
            if redirect_url and not redirect_url.startswith(('http://', 'https://')):
                # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π URL –¥–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞
                parsed_url = urlparse(url)
                base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
                redirect_url = f"{base_url}{redirect_url if redirect_url.startswith('/') else '/' + redirect_url}"

            if redirect_url:
                logger.info(f"–†–µ–¥–∏—Ä–µ–∫—Ç {redirect_count+1}/{MAX_REDIRECTS}: {url} -> {redirect_url}")

                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π page_info —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º URL
                new_page_info = page_info.copy()
                new_page_info['page_url'] = redirect_url

                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
                return fetch_page_content(new_page_info, retry, redirect_count + 1)

        # –ï—Å–ª–∏ –±—ã–ª 3xx/4xx/5xx —Å—Ç–∞—Ç—É—Å, –Ω–æ –Ω–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç
        response.raise_for_status()

        html_content = response.text

        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'id_page': page_info['id_page'],
            'page_url': page_info['page_url'],
            'publication_date': page_info.get('publication_date'),
            'page_raw_content': html_content,
            'content_size': len(html_content)
        }

        return result
    except requests.exceptions.RequestException as e:
        # –ï—Å–ª–∏ –Ω–µ –ø—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫, –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞
        if retry < MAX_RETRIES - 1:
            logger.warning(f"Error loading {url}, attempt {retry + 1}/{MAX_RETRIES}: {e}")
            # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —Å –Ω–µ–±–æ–ª—å—à–æ–π —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å—é
            delay = (2 ** retry) + random.uniform(0, 1)
            time.sleep(delay)
            return fetch_page_content(page_info, retry + 1, redirect_count)

        logger.error(f"Failed to load page {url} after {MAX_RETRIES} attempts: {e}")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –æ—à–∏–±–∫–∏ –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        config = load_config()
        conn = get_db_connection(config)
        update_page_error_status(conn, page_info['id_page'], True)
        conn.close()

        return {
            'id_page': page_info['id_page'],
            'page_url': page_info['page_url'],
            'publication_date': page_info.get('publication_date'),
            'page_raw_content': None,
            'content_size': 0,
            'is_error': True
        }

# ---------------------------------------------------
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏
# ---------------------------------------------------

def process_page_raw_content(page, db_config):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞
    conn = get_db_connection(db_config)

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_info = fetch_page_content(page)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –ë–î
    if content_info and 'id_page' in content_info:
        success = save_raw_content_to_db(conn, content_info)
        domain = page['domain']

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if success and content_info['page_raw_content']:
            stats['successful_raw_pages'] += 1
            stats['domains_stats'][domain]['success'] += 1
            content_size = content_info['content_size']
            stats['total_content_size'] += content_size
            stats['domains_stats'][domain]['size'] += content_size
        else:
            stats['failed_raw_pages'] += 1
            stats['domains_stats'][domain]['failed'] += 1

            # –ï—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ –æ—à–∏–±–∫–∏, –æ–±–Ω–æ–≤–∏–º —Å—Ç–∞—Ç—É—Å –≤ –ë–î
            if content_info.get('is_error', False):
                update_page_error_status(conn, content_info['id_page'], True)
    else:
        success = False
        stats['failed_raw_pages'] += 1
        domain = page['domain']
        stats['domains_stats'][domain]['failed'] += 1

    # –ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    conn.close()

    # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä
    time.sleep(random.uniform(0.5, 2.0))

    return success

def process_page_content_extraction(page, db_config):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Å—ã—Ä–æ–≥–æ HTML –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î"""
    # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞
    conn = get_db_connection(db_config)

    # –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏ —Ç–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    raw_content, current_content = get_raw_content_from_db(conn, page['id_page'])

    if not raw_content:
        logger.error(f"No raw content found for page ID={page['id_page']}")
        conn.close()
        return False

    # –ì–æ—Ç–æ–≤–∏–º —Å–ª–æ–≤–∞—Ä—å —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
    extracted_content = {}

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if current_content is None:
        content = extract_content(raw_content)
        extracted_content['content'] = content
        stats['parser_stats']['trafilatura']['parsed'] += 1 if content else 0

    # –ï—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    if extracted_content:
        extracted_content['id_page'] = page['id_page']
        success = save_extracted_content_to_db(conn, extracted_content)
    else:
        logger.info(f"No content to extract for page ID={page['id_page']}")
        success = True

    # –ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    conn.close()

    return success

def process_pages(pages, process_func, db_config):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ThreadPoolExecutor"""
    results = []

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ –¥–æ–º–µ–Ω–∞–º –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –æ–¥–Ω–æ–º—É –¥–æ–º–µ–Ω—É
    domain_pages = defaultdict(list)
    for page in pages:
        domain_pages[page['domain']].append(page)

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–º–µ–Ω –æ—Ç–¥–µ–ª—å–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # –î–µ—Ä–∂–∏–º —É—á–µ—Ç –≤—Å–µ—Ö –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        all_futures = []

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        for domain, domain_page_list in domain_pages.items():
            # –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–º–µ–Ω–∞ –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏
            batches = [domain_page_list[i:i + MAX_CONCURRENT_PER_DOMAIN]
                     for i in range(0, len(domain_page_list), MAX_CONCURRENT_PER_DOMAIN)]

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
            for batch in batches:
                batch_futures = []
                for page in batch:
                    future = executor.submit(process_func, page, db_config)
                    batch_futures.append(future)
                    all_futures.append((future, page))

                # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ
                for future in concurrent.futures.as_completed(batch_futures):
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"Exception in batch processing: {e}")

                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞
                time.sleep(random.uniform(0.2, 0.5))

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –∑–∞–¥–∞—á
        for future, page in all_futures:
            try:
                result = future.result()
                results.append(result)
                logger.info(f"Completed processing page: {page['page_url']}")
            except Exception as e:
                logger.error(f"Exception processing page {page['page_url']}: {e}")
                results.append(False)

    return results

# ---------------------------------------------------
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
# ---------------------------------------------------

def format_size(size_bytes):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –≤ —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.2f} KB"
    elif size_bytes < 1024 * 1024 * 1024:
        return f"{size_bytes / (1024 * 1024):.2f} MB"
    else:
        return f"{size_bytes / (1024 * 1024 * 1024):.2f} GB"

def format_time(seconds):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤ —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
    if seconds < 60:
        return f"{seconds:.2f} seconds"
    elif seconds < 3600:
        minutes = seconds // 60
        sec = seconds % 60
        return f"{int(minutes)} min {int(sec)} sec"
    else:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        sec = seconds % 60
        return f"{int(hours)} hours {int(minutes)} min {int(sec)} sec"

def display_statistics():
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã —Å–∫—Ä–∏–ø—Ç–∞"""
    if not stats['start_time'] or not stats['end_time']:
        return

    execution_time = (stats['end_time'] - stats['start_time']).total_seconds()

    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
    success_rate_raw = "0%"
    if stats['total_raw_pages'] > 0:
        success_rate_raw = f"{(stats['successful_raw_pages'] / stats['total_raw_pages'] * 100):.2f}%"

    avg_page_size = "0 B"
    if stats['successful_raw_pages'] > 0:
        avg_page_size = format_size(stats['total_content_size'] / stats['successful_raw_pages'])

    pages_per_second = "0"
    if execution_time > 0:
        pages_per_second = f"{(stats['total_raw_pages'] + stats['total_parsed_pages']) / execution_time:.2f}"

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    general_stats = [
        ["Execution time", format_time(execution_time)],
        ["Total raw pages processed", stats['total_raw_pages']],
        ["Successful raw pages", stats['successful_raw_pages']],
        ["Failed raw pages", stats['failed_raw_pages']],
        ["Total pages parsed", stats['total_parsed_pages']],
        ["Success rate (raw)", success_rate_raw],
        ["Total content size", format_size(stats['total_content_size'])],
        ["Average page size", avg_page_size],
        ["Pages per second", pages_per_second]
    ]

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º
    domains_stats = []
    for domain, domain_stats in sorted(stats['domains_stats'].items()):
        total = domain_stats['success'] + domain_stats['failed']
        success_rate = 0
        if total > 0:
            success_rate = (domain_stats['success'] / total * 100)
        domains_stats.append([
            domain,
            domain_stats['success'],
            domain_stats['failed'],
            f"{success_rate:.2f}%",
            format_size(domain_stats['size'])
        ])

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–∞—Ä—Å–µ—Ä–∞–º
    parser_stats = []
    for parser_name, parser_data in stats['parser_stats'].items():
        total = parser_data['success'] + parser_data['failed']
        success_rate = "0%"
        if total > 0:
            success_rate = f"{(parser_data['success'] / total * 100):.2f}%"
        parser_stats.append([
            parser_name.capitalize(),
            parser_data['success'],
            parser_data['failed'],
            parser_data['parsed'],
            success_rate
        ])

    # –í—ã–≤–æ–¥–∏–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\n‚úÖ GENERAL STATISTICS:")
    print(tabulate(general_stats, tablefmt="pretty"))

    print("\nüåê DOMAIN STATISTICS:")
    print(tabulate(domains_stats, headers=["Domain", "Success", "Failed", "Success Rate", "Content Size"], tablefmt="pretty"))

    print("\nüìë PARSER STATISTICS:")
    print(tabulate(parser_stats, headers=["Parser", "Success", "Failed", "Parsed", "Success Rate"], tablefmt="pretty"))

# ---------------------------------------------------
# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
# ---------------------------------------------------

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats['start_time'] = datetime.now()

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    setup_logging()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    db_config = load_config()

    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    conn = get_db_connection(db_config)

    # –≠—Ç–∞–ø 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—ã—Ä–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    logger.info("Stage 1: Fetching raw content")
    pages_for_raw = get_pages_for_raw_content(conn)
    stats['total_raw_pages'] = len(pages_for_raw)
    logger.info(f"Found {len(pages_for_raw)} pages for raw content fetching")

    if pages_for_raw:
        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—ã—Ä–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        results_raw = process_pages(pages_for_raw, process_page_raw_content, db_config)
        success_count_raw = results_raw.count(True)
        logger.info(f"Processed {len(pages_for_raw)} raw pages, successful: {success_count_raw}, with errors: {len(pages_for_raw) - success_count_raw}")
    else:
        logger.info("No pages for raw content fetching")

    # –≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    logger.info("Stage 2: Extracting content")
    pages_for_extraction = get_pages_for_content_extraction(conn)
    stats['total_parsed_pages'] = len(pages_for_extraction)
    logger.info(f"Found {len(pages_for_extraction)} pages for content extraction")

    if pages_for_extraction:
        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        results_extraction = process_pages(pages_for_extraction, process_page_content_extraction, db_config)
        success_count_extraction = results_extraction.count(True)
        logger.info(f"Processed {len(pages_for_extraction)} pages for extraction, successful: {success_count_extraction}, with errors: {len(pages_for_extraction) - success_count_extraction}")
    else:
        logger.info("No pages for content extraction")

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
    conn.close()

    # –§–∏–∫—Å–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
    stats['end_time'] = datetime.now()

    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    display_statistics()

    logger.info("Script executed successfully")

if __name__ == "__main__":
    main()